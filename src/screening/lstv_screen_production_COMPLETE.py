#!/usr/bin/env python3
"""
PRODUCTION LSTV SCREENING SYSTEM v3.0 - CSV IS ABSOLUTE TRUTH

Changes in this version:
1. CSV descriptions are trusted completely for series selection
2. No DICOM header verification during series selection
3. All NIfTI files are processed regardless of orientation
4. Helper functions preserved but not called in main logic

Author: Claude + go2432
Date: February 2026
"""

import argparse
import json
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import cv2
import nibabel as nib
from tqdm import tqdm
import subprocess
import shutil
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional
import os


# ============================================================================
# REORIENTATION FUNCTIONS (PRESERVED BUT NOT USED IN MAIN LOGIC)
# ============================================================================

def reorient_to_standard(nifti_path, acquisition_plane):
    """Reorient NIfTI to standard radiological orientation
    
    NOTE: This function is preserved for potential future use but is NOT
    called in the current production logic.
    """
    try:
        from nibabel.orientations import axcodes2ornt, ornt_transform

        img = nib.load(nifti_path)
        original_axcodes = nib.aff2axcodes(img.affine)

        standard_orientations = {
            'sagittal': ('L', 'A', 'S'),
            'coronal': ('R', 'S', 'A'),
            'axial': ('R', 'A', 'S')
        }

        if acquisition_plane not in standard_orientations:
            return nifti_path, False

        target_axcodes = standard_orientations[acquisition_plane]

        if original_axcodes == target_axcodes:
            print(f"    âœ“ Already standard {acquisition_plane}: {original_axcodes}")
            return nifti_path, True

        print(f"    â†» Reorienting: {original_axcodes} â†’ {target_axcodes}")

        original_ornt = nib.orientations.io_orientation(img.affine)
        target_ornt = axcodes2ornt(target_axcodes)
        transform = ornt_transform(original_ornt, target_ornt)

        reoriented_img = img.as_reoriented(transform)
        nib.save(reoriented_img, nifti_path)

        verify_img = nib.load(nifti_path)
        verify_axcodes = nib.aff2axcodes(verify_img.affine)

        if verify_axcodes == target_axcodes:
            print(f"    âœ“ Reoriented: {verify_axcodes}")
            return nifti_path, True
        else:
            print(f"    âœ— Failed: {verify_axcodes} != {target_axcodes}")
            return nifti_path, False

    except Exception as e:
        print(f"    âœ— Reorientation error: {e}")
        return nifti_path, False


def determine_acquisition_plane_from_csv(series_description):
    """Determine acquisition plane from CSV - CSV IS TRUTH"""
    desc_lower = series_description.lower()

    if 'sagittal' in desc_lower or 'sag ' in desc_lower:
        return 'sagittal'
    elif 'coronal' in desc_lower or 'cor ' in desc_lower:
        return 'coronal'
    elif 'axial' in desc_lower or 'ax ' in desc_lower:
        return 'axial'

    return None


def get_acquisition_plane_from_dicom(dicom_dir):
    """
    Determine the actual acquisition plane from DICOM headers
    
    NOTE: This function is preserved for potential future use but is NOT
    called in the current production logic where CSV is absolute truth.

    Reads ImageOrientationPatient tag to determine how slices were acquired,
    regardless of how the 3D volume is stored.

    Returns: 'sagittal', 'coronal', 'axial', or None
    """
    try:
        import pydicom

        # Find first DICOM file
        dicom_files = list(Path(dicom_dir).glob('*.dcm'))
        if not dicom_files:
            dicom_files = list(Path(dicom_dir).glob('*'))
            if not dicom_files:
                return None

        # Try to read first file
        try:
            dcm = pydicom.dcmread(str(dicom_files[0]), stop_before_pixels=True)
        except:
            return None

        # Get ImageOrientationPatient (0020,0037)
        if not hasattr(dcm, 'ImageOrientationPatient'):
            return None

        iop = dcm.ImageOrientationPatient
        if len(iop) != 6:
            return None

        # iop contains two 3D vectors:
        # [row_x, row_y, row_z, col_x, col_y, col_z]
        # These describe the direction of rows and columns in patient space

        import numpy as np
        row_vec = np.array([float(iop[0]), float(iop[1]), float(iop[2])])
        col_vec = np.array([float(iop[3]), float(iop[4]), float(iop[5])])

        # Cross product gives slice normal direction
        slice_normal = np.cross(row_vec, col_vec)
        slice_normal = slice_normal / np.linalg.norm(slice_normal)

        # Determine which axis is dominant
        abs_normal = np.abs(slice_normal)
        dominant_axis = np.argmax(abs_normal)

        # 0=x (sagittal), 1=y (coronal), 2=z (axial)
        if dominant_axis == 0:
            return 'sagittal'
        elif dominant_axis == 1:
            return 'coronal'
        else:
            return 'axial'

    except Exception as e:
        return None


# ============================================================================
# CONSTANTS
# ============================================================================

# Semantic labels (WITH ribs and TPs!)
SEMANTIC_LABELS = {
    'spinal_cord': 1,
    'spinal_canal': 2,
    'vertebra_corpus': 3,
    'vertebra_disc': 4,
    'endplate': 5,
    'arcus_vertebrae': 6,
    'rib_left': 7,              # â† T12 RIBS
    'rib_right': 8,             # â† T12 RIBS
    'transverse_process_left': 9,   # â† L5 TPs
    'transverse_process_right': 10, # â† L5 TPs
    'spinosus_process': 11,
}

# Instance labels
INSTANCE_LABELS = {
    'C1': 1, 'C2': 2, 'C3': 3, 'C4': 4, 'C5': 5, 'C6': 6, 'C7': 7,
    'T1': 8, 'T2': 9, 'T3': 10, 'T4': 11, 'T5': 12, 'T6': 13,
    'T7': 14, 'T8': 15, 'T9': 16, 'T10': 17, 'T11': 18, 'T12': 19,
    'L1': 20, 'L2': 21, 'L3': 22, 'L4': 23, 'L5': 24, 'L6': 25,
    'Sacrum': 26,
    'T12_L1_disc': 119,
    'L1_L2_disc': 120,
    'L2_L3_disc': 121,
    'L3_L4_disc': 122,
    'L4_L5_disc': 123,
    'L5_S1_disc': 124,
    'S1_S2_disc': 126,
}

ID_TO_NAME = {v: k for k, v in INSTANCE_LABELS.items()}

LSTV_COLORS = {
    'L6': (255, 0, 255),          # Magenta - LUMBARIZATION
    'S1_S2_disc': (255, 128, 0),  # Orange - SACRALIZATION
    'L5': (0, 255, 0),            # Green - Normal L5
    'Sacrum': (0, 255, 255),      # Cyan - Sacrum
    'L4': (100, 255, 100),        # Light green
    'default': (255, 255, 0),     # Yellow - Other vertebrae
}

@dataclass
class DetectionResult:
    structure_type: str  # 'T12_rib', 'L5_TP'
    side: str  # 'left', 'right'
    bbox: Tuple[int, int, int, int]
    confidence: float
    slice_idx: int
    view: str
    detection_method: str  # 'semantic', 'intensity'
    area: int

# ============================================================================
# SPINE-AWARE SLICE SELECTOR
# ============================================================================

class SpineAwareSliceSelector:
    """Intelligent slice selection using spine segmentation"""

    def __init__(self, voxel_spacing_mm=1.0, parasagittal_offset_mm=30):
        self.voxel_spacing_mm = voxel_spacing_mm
        self.parasagittal_offset_mm = parasagittal_offset_mm

    def find_sagittal_axis(self, data_shape):
        """Determine sagittal axis (smallest dimension)"""
        return np.argmin(data_shape)

    def calculate_spine_density(self, seg_data, sag_axis, slice_idx):
        """Calculate spine content in a slice"""
        lumbar_labels = [20, 21, 22, 23, 24, 26]  # L1-L5 + Sacrum

        vertebra_mask = np.zeros_like(seg_data, dtype=bool)
        for label in lumbar_labels:
            vertebra_mask |= (seg_data == label)

        if sag_axis == 0:
            slice_mask = vertebra_mask[slice_idx, :, :]
        elif sag_axis == 1:
            slice_mask = vertebra_mask[:, slice_idx, :]
        else:
            slice_mask = vertebra_mask[:, :, slice_idx]

        return slice_mask.sum()

    def find_optimal_midline(self, seg_data, sag_axis):
        """Find TRUE spinal midline using segmentation"""
        num_slices = seg_data.shape[sag_axis]
        geometric_mid = num_slices // 2

        lumbar_labels = [20, 21, 22, 23, 24, 26]

        vertebra_mask = np.zeros_like(seg_data, dtype=bool)
        for label in lumbar_labels:
            if label in seg_data:
                vertebra_mask |= (seg_data == label)

        if not vertebra_mask.any():
            return geometric_mid

        spine_density = np.zeros(num_slices)
        for i in range(num_slices):
            if sag_axis == 0:
                slice_mask = vertebra_mask[i, :, :]
            elif sag_axis == 1:
                slice_mask = vertebra_mask[:, i, :]
            else:
                slice_mask = vertebra_mask[:, :, i]
            spine_density[i] = slice_mask.sum()

        optimal_mid = int(np.argmax(spine_density))
        return optimal_mid

    def get_three_slices(self, seg_data, sag_axis):
        """Get left, mid, right slice indices"""
        optimal_mid = self.find_optimal_midline(seg_data, sag_axis)

        num_slices = seg_data.shape[sag_axis]
        offset_voxels = int(self.parasagittal_offset_mm / self.voxel_spacing_mm)

        left_idx = max(0, optimal_mid - offset_voxels)
        right_idx = min(num_slices - 1, optimal_mid + offset_voxels)

        return {
            'left': left_idx,
            'mid': optimal_mid,
            'right': right_idx,
            'sag_axis': sag_axis,
        }


# ============================================================================
# HELPER FUNCTIONS - FIXED CSV LOADING
# ============================================================================

def load_series_descriptions(csv_path):
    """Load series descriptions CSV with proper dtypes"""
    try:
        df = pd.read_csv(csv_path)

        # Check required columns
        required_cols = ['study_id', 'series_id', 'series_description']
        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            print(f"  âœ— CSV missing columns: {missing}")
            print(f"  Available columns: {list(df.columns)}")
            return None

        # Convert dtypes
        df['study_id'] = df['study_id'].astype(int)
        df['series_id'] = df['series_id'].astype(int)  # Series IDs are integers
        df['series_description'] = df['series_description'].astype(str)

        print(f"  âœ“ Loaded {len(df)} series descriptions")
        print(f"  âœ“ Unique studies: {df['study_id'].nunique()}")
        print(f"  âœ“ Sample descriptions:")
        desc_counts = df['series_description'].value_counts().head(5)
        for desc, count in desc_counts.items():
            print(f"      - {desc}: {count}")

        return df
    except Exception as e:
        print(f"  âœ— Failed to load series CSV: {e}")
        import traceback
        traceback.print_exc()
        return None


def select_best_series(study_dir, series_df=None, study_id=None):
    """
    Select best T2 SAGITTAL series - CSV IS ABSOLUTE TRUTH
    
    Strategy:
    1. Trust CSV descriptions completely - NO DICOM header verification
    2. Look for "Sagittal T2" patterns in CSV
    3. Return prioritized list for multi-series fallback
    """
    series_dirs = sorted([d for d in study_dir.iterdir() if d.is_dir()])
    if not series_dirs:
        return None

    # If no CSV, return all series in order
    if series_df is None or study_id is None:
        print(f"    âš  No CSV - returning all series")
        return series_dirs

    try:
        study_id_int = int(study_id)
        study_series = series_df[series_df['study_id'] == study_id_int].copy()
    except (ValueError, TypeError) as e:
        print(f"    âš  Invalid study_id: {e}")
        return series_dirs

    if len(study_series) == 0:
        print(f"    âš  Study not in CSV")
        return series_dirs

    # Get series IDs that exist in filesystem
    available_series_ids = [int(d.name) for d in series_dirs]
    study_series = study_series[study_series['series_id'].isin(available_series_ids)]

    if len(study_series) == 0:
        print(f"    âš  No CSV series in filesystem")
        return series_dirs

    priority_order = []

    # CSV IS TRUTH - trust the descriptions completely
    sagittal_patterns = [
        'Sagittal T2/STIR',
        'Sagittal T2',
        'SAG T2 STIR',
        'SAG T2',
    ]

    for pattern in sagittal_patterns:
        matching = study_series[
            study_series['series_description'].str.contains(
                pattern, case=False, na=False, regex=False)
        ]
        for _, row in matching.iterrows():
            series_id = int(row['series_id'])
            series_path = study_dir / str(series_id)
            if series_path.exists() and series_path not in priority_order:
                priority_order.append(series_path)
                print(f"    âœ“ CSV says sagittal T2: {row['series_description']}")

    # Add remaining series as fallback
    for series_path in series_dirs:
        if series_path not in priority_order:
            priority_order.append(series_path)

    if len(priority_order) == 0:
        return None

    return priority_order


def convert_dicom_to_nifti(dicom_dir, output_path, study_id, verify_sagittal=True):
    """
    Convert DICOM to NIfTI with optional orientation checking

    Args:
        verify_sagittal: If True, returns None for non-sagittal images
                        If False, converts anyway (CSV IS TRUTH mode)

    Returns:
        (nifti_path, orientation) tuple, or (None, None) on failure
    """
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Use BIDS-like naming
        bids_base = f"sub-{study_id}_sequ-sag_T2w"

        # dcm2niix with orientation preservation
        cmd = [
            'dcm2niix',
            '-z', 'y',
            '-f', bids_base,
            '-o', str(output_path.parent),
            '-m', 'y',
            '-ba', 'n',
            '-i', 'n',
            '-x', 'n',
            '-p', 'n',
            str(dicom_dir)
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

        if result.returncode != 0:
            return (None, None)

        # Find generated NIfTI
        expected_output = output_path.parent / f"{bids_base}.nii.gz"
        if not expected_output.exists():
            nifti_files = sorted(output_path.parent.glob(f"{bids_base}*.nii.gz"))
            if not nifti_files:
                nifti_files = sorted(output_path.parent.glob(f"sub-{study_id}*.nii.gz"))
            if not nifti_files:
                return (None, None)

            generated_file = nifti_files[0]
            if generated_file != expected_output:
                shutil.move(str(generated_file), str(expected_output))

        # Check orientation
        nii = nib.load(expected_output)
        orientation = nib.aff2axcodes(nii.affine)

        first_axis = orientation[0]
        is_sagittal = (first_axis in ('R', 'L'))

        if verify_sagittal and not is_sagittal:
            # Clean up non-sagittal file
            expected_output.unlink()
            return (None, orientation)

        return (expected_output, orientation)

    except subprocess.TimeoutExpired:
        return (None, None)
    except Exception as e:
        return (None, None)


# ============================================================================
# SPINEPS DUAL EXTRACTOR
# ============================================================================

def run_spineps_dual_extraction(nifti_path: Path, output_dir: Path) -> Optional[Dict[str, Path]]:
    """
    Run SPINEPS and extract BOTH instance and semantic outputs

    Returns:
        {
            'instance': Path,
            'semantic': Path or None
        }
    """
    try:
        output_dir.mkdir(parents=True, exist_ok=True)

        # Set environment
        env = os.environ.copy()
        env['SPINEPS_SEGMENTOR_MODELS'] = env.get('SPINEPS_SEGMENTOR_MODELS', '/app/models')
        env['SPINEPS_ENVIRONMENT_DIR'] = env.get('SPINEPS_ENVIRONMENT_DIR', '/app/models')

        # Find wrapper script
        script_dir = Path(__file__).parent
        wrapper_candidates = [
            script_dir / 'spineps_wrapper_FIXED.sh',
            script_dir / 'spineps_wrapper.sh',
            Path('/work/src/screening/spineps_wrapper_FIXED.sh'),
            Path('/work/src/screening/spineps_wrapper.sh'),
        ]

        wrapper_path = None
        for candidate in wrapper_candidates:
            if candidate.exists():
                wrapper_path = candidate
                print(f"    âœ“ Found wrapper: {wrapper_path}")
                break
            else:
                print(f"    âœ— Wrapper not found: {candidate}")

        if wrapper_path:
            print(f"    Running SPINEPS via wrapper: {wrapper_path.name}")
            sys.stdout.flush()

            cmd = [
                'bash', str(wrapper_path), 'sample',
                '-i', str(nifti_path),
                '-model_semantic', 't2w',
                '-model_instance', 'instance',
                '-model_labeling', 't2w_labeling',
                '-override_semantic', '-override_instance', '-override_ctd'
            ]
        else:
            print(f"    Running SPINEPS via Python module")
            sys.stdout.flush()

            cmd = [
                'python', '-m', 'spineps.entrypoint', 'sample',
                '-i', str(nifti_path),
                '-model_semantic', 't2w',
                '-model_instance', 'instance',
                '-model_labeling', 't2w_labeling',
                '-override_semantic', '-override_instance', '-override_ctd'
            ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600, env=env)

        # Print SPINEPS output for debugging
        if result.stdout:
            stdout_lines = [line for line in result.stdout.split('\n') if line.strip()]
            if stdout_lines and len(stdout_lines) <= 30:
                print(f"    SPINEPS output:")
                for line in stdout_lines[-20:]:
                    print(f"      {line}")

        if result.returncode != 0:
            print(f"  âœ— SPINEPS failed (code {result.returncode})")
            return None

        # SPINEPS creates derivatives_seg/ next to INPUT file
        input_parent = nifti_path.parent
        derivatives_dir = input_parent / "derivatives_seg"

        if not derivatives_dir.exists():
            print(f"  âœ— derivatives_seg not found at {derivatives_dir}")
            return None

        # Extract study ID from filename
        study_id = nifti_path.stem.replace('_sequ-sag_T2w', '').replace('.nii', '').replace('sub-', '')

        # Find instance mask
        seg_pattern = f"sub-{study_id}_*_seg-vert_msk.nii.gz"
        instance_files = list(derivatives_dir.glob(seg_pattern))

        if not instance_files:
            instance_files = list(derivatives_dir.glob("*_seg-vert_msk.nii.gz"))

        if not instance_files:
            print(f"  âœ— Instance mask not found")
            return None

        instance_file = instance_files[0]
        print(f"    âœ“ Found instance: {instance_file.name}")

        # Find semantic mask
        semantic_pattern = f"sub-{study_id}_*_seg-spine_msk.nii.gz"
        semantic_files = list(derivatives_dir.glob(semantic_pattern))

        if not semantic_files:
            semantic_files = list(derivatives_dir.glob("*_seg-spine_msk.nii.gz"))

        semantic_file = semantic_files[0] if semantic_files else None

        if semantic_file:
            print(f"    âœ“ Found semantic: {semantic_file.name}")

        # Copy to output directory
        instance_output = output_dir / f"{study_id}_instance.nii.gz"
        shutil.copy(instance_file, instance_output)

        outputs = {'instance': instance_output}

        if semantic_file:
            semantic_output = output_dir / f"{study_id}_semantic.nii.gz"
            shutil.copy(semantic_file, semantic_output)
            outputs['semantic'] = semantic_output
            print(f"  âœ“ Extracted: instance + semantic")
        else:
            print(f"  âš  Extracted: instance only")

        return outputs

    except subprocess.TimeoutExpired:
        print(f"  âœ— SPINEPS timeout (>600s)")
        return None
    except Exception as e:
        print(f"  âœ— SPINEPS failed: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================================
# CONFIDENCE SCORING
# ============================================================================

def calculate_lstv_confidence(seg_data, unique_labels, lumbar_labels,
                               has_l6, s1_s2_disc, has_sacrum):
    """Calculate confidence score for LSTV detection"""
    confidence_score = 0.0
    confidence_factors = []

    # Factor 1: L6 size validation
    if has_l6:
        l6_mask = (seg_data == 25)
        l5_mask = (seg_data == 24) if 24 in unique_labels else None

        l6_volume = l6_mask.sum()

        if l5_mask is not None:
            l5_volume = l5_mask.sum()
            size_ratio = l6_volume / l5_volume if l5_volume > 0 else 0

            if 0.5 <= size_ratio <= 1.5:
                confidence_score += 0.4
                confidence_factors.append(f"L6/L5 ratio: {size_ratio:.2f} (valid)")
            else:
                confidence_factors.append(f"L6/L5 ratio: {size_ratio:.2f} (SUSPICIOUS)")

        if l6_volume > 500:
            confidence_score += 0.2
            confidence_factors.append(f"L6 volume: {l6_volume} voxels (OK)")
        else:
            confidence_factors.append(f"L6 volume: {l6_volume} voxels (TOO SMALL)")

    # Factor 2: Sacrum presence
    if has_sacrum:
        confidence_score += 0.2
        confidence_factors.append("Sacrum detected")
    else:
        confidence_factors.append("NO SACRUM (red flag)")

    # Factor 3: S1-S2 disc
    if s1_s2_disc:
        confidence_score += 0.3
        confidence_factors.append("S1-S2 disc (strong evidence)")

    # Factor 4: Vertebra count
    vertebra_count = len(lumbar_labels)
    if vertebra_count in [4, 5, 6]:
        confidence_score += 0.1
        confidence_factors.append(f"Count: {vertebra_count} (plausible)")
    else:
        confidence_factors.append(f"Count: {vertebra_count} (IMPLAUSIBLE)")

    if confidence_score >= 0.7:
        confidence_level = "HIGH"
    elif confidence_score >= 0.4:
        confidence_level = "MEDIUM"
    else:
        confidence_level = "LOW"

    return confidence_score, confidence_level, confidence_factors


def analyze_segmentation(seg_path):
    """Analyze segmentation for LSTV candidates"""
    try:
        seg_nii = nib.load(seg_path)
        seg_data = seg_nii.get_fdata().astype(int)
        unique_labels = np.unique(seg_data)
        vertebra_labels = [l for l in unique_labels if 1 <= l <= 25]
        lumbar_labels = [l for l in vertebra_labels if 20 <= l <= 25]

        vertebra_count = len(lumbar_labels)
        has_sacrum = 26 in unique_labels
        has_l6 = 25 in lumbar_labels
        s1_s2_disc = 126 in unique_labels
        is_lstv = (vertebra_count != 5 or s1_s2_disc or has_l6)

        lstv_type = "normal"
        if vertebra_count < 5:
            lstv_type = "sacralization"
        elif vertebra_count > 5 or has_l6:
            lstv_type = "lumbarization"
        elif s1_s2_disc:
            lstv_type = "s1_s2_disc"

        if is_lstv:
            confidence_score, confidence_level, confidence_factors = calculate_lstv_confidence(
                seg_data, unique_labels, lumbar_labels, has_l6, s1_s2_disc, has_sacrum
            )
        else:
            confidence_score = 0.0
            confidence_level = "N/A"
            confidence_factors = []

        return {
            'vertebra_count': vertebra_count,
            'has_sacrum': has_sacrum,
            'has_l6': has_l6,
            's1_s2_disc': s1_s2_disc,
            'is_lstv_candidate': is_lstv,
            'lstv_type': lstv_type,
            'unique_labels': list(map(int, unique_labels)),
            'lumbar_labels': list(map(int, lumbar_labels)),
            'confidence_score': round(confidence_score, 2),
            'confidence_level': confidence_level,
            'confidence_factors': confidence_factors,
        }
    except Exception as e:
        print(f"  âœ— Analysis error: {e}")
        return None


# ============================================================================
# PARASAGITTAL OPTIMIZER
# ============================================================================

def find_optimal_parasagittal_slices_semantic(
    semantic_data: np.ndarray,
    instance_data: np.ndarray,
    target_vertebra: str,
    voxel_spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0)
) -> Dict[str, int]:
    """Find parasagittal slices with max rib/TP density"""

    sag_axis = np.argmin(semantic_data.shape)
    num_slices = semantic_data.shape[sag_axis]

    if target_vertebra == 'T12':
        left_label = SEMANTIC_LABELS['rib_left']
        right_label = SEMANTIC_LABELS['rib_right']
        vertebra_instance = INSTANCE_LABELS['T12']
    else:  # L5
        left_label = SEMANTIC_LABELS['transverse_process_left']
        right_label = SEMANTIC_LABELS['transverse_process_right']
        vertebra_instance = INSTANCE_LABELS['L5']

    vertebra_mask = (instance_data == vertebra_instance)

    if not vertebra_mask.any():
        center = num_slices // 2
        offset = 40
        return {
            'left': max(0, center - offset),
            'right': min(num_slices - 1, center + offset),
            'left_density': 0,
            'right_density': 0
        }

    if sag_axis == 0:
        coords = np.where(vertebra_mask)[0]
    elif sag_axis == 1:
        coords = np.where(vertebra_mask)[1]
    else:
        coords = np.where(vertebra_mask)[2]

    center = int(np.median(coords))
    offset = int(40 / voxel_spacing[sag_axis])
    search_range = 15

    # Left side
    left_densities = []
    for i in range(max(0, center - offset - search_range),
                  min(num_slices, center - offset + search_range + 1)):
        if sag_axis == 0:
            slice_data = semantic_data[i, :, :]
        elif sag_axis == 1:
            slice_data = semantic_data[:, i, :]
        else:
            slice_data = semantic_data[:, :, i]

        density = (slice_data == left_label).sum()
        left_densities.append((i, density))

    # Right side
    right_densities = []
    for i in range(max(0, center + offset - search_range),
                  min(num_slices, center + offset + search_range + 1)):
        if sag_axis == 0:
            slice_data = semantic_data[i, :, :]
        elif sag_axis == 1:
            slice_data = semantic_data[:, i, :]
        else:
            slice_data = semantic_data[:, :, i]

        density = (slice_data == right_label).sum()
        right_densities.append((i, density))

    left_optimal, left_max = max(left_densities, key=lambda x: x[1])
    right_optimal, right_max = max(right_densities, key=lambda x: x[1])

    return {
        'left': left_optimal,
        'right': right_optimal,
        'left_density': left_max,
        'right_density': right_max
    }


# ============================================================================
# IMAGE PROCESSING
# ============================================================================

def extract_slice(data, sag_axis, slice_idx, thickness=1):
    """Extract 2D slice from 3D volume"""
    if thickness <= 1:
        if sag_axis == 0:
            return data[slice_idx, :, :]
        elif sag_axis == 1:
            return data[:, slice_idx, :]
        else:
            return data[:, :, slice_idx]

    half_thick = thickness // 2
    start = max(0, slice_idx - half_thick)
    end = min(data.shape[sag_axis], slice_idx + half_thick + 1)

    if sag_axis == 0:
        slab = data[start:end, :, :]
    elif sag_axis == 1:
        slab = data[:, start:end, :]
    else:
        slab = data[:, :, start:end]

    return np.max(slab, axis=sag_axis)


def normalize_slice(img_slice):
    """Normalize to 0-255 uint8 with CLAHE"""
    if img_slice.max() > img_slice.min():
        normalized = ((img_slice - img_slice.min()) /
                     (img_slice.max() - img_slice.min()) * 255)
        normalized = normalized.astype(np.uint8)

        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        normalized = clahe.apply(normalized)
        return normalized
    return np.zeros_like(img_slice, dtype=np.uint8)


def get_vertebra_centroid(seg_slice, label_id):
    """Get center of mass for a vertebra"""
    mask = (seg_slice == label_id)
    if not mask.any():
        return None

    coords = np.argwhere(mask)
    cy = int(coords[:, 0].mean())
    cx = int(coords[:, 1].mean())
    return (cx, cy)


def create_labeled_overlay(mri_slice, seg_slice, lstv_info):
    """Create RGB image with vertebra labels"""
    rgb_img = cv2.cvtColor(mri_slice, cv2.COLOR_GRAY2RGB)

    unique_labels = np.unique(seg_slice)
    vertebrae = [l for l in unique_labels if l in ID_TO_NAME]
    vertebrae_sorted = sorted(vertebrae)

    for label_id in vertebrae_sorted:
        name = ID_TO_NAME[label_id]
        centroid = get_vertebra_centroid(seg_slice, label_id)

        if centroid is None:
            continue

        cx, cy = centroid

        if name == 'L6':
            color = LSTV_COLORS['L6']
            thickness = 3
            font_scale = 0.9
        elif name == 'S1_S2_disc':
            color = LSTV_COLORS['S1_S2_disc']
            thickness = 3
            font_scale = 0.7
        elif name == 'L5':
            color = LSTV_COLORS['L5']
            thickness = 2
            font_scale = 0.8
        elif name == 'Sacrum':
            color = LSTV_COLORS['Sacrum']
            thickness = 2
            font_scale = 0.8
        else:
            color = LSTV_COLORS['default']
            thickness = 1
            font_scale = 0.6

        cv2.putText(rgb_img, name, (cx - 30, cy),
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale,
                   color, thickness, cv2.LINE_AA)
        cv2.circle(rgb_img, (cx, cy), 4, color, -1)

    # Add LSTV banner
    if lstv_info['is_lstv_candidate']:
        banner_height = 40
        banner = np.zeros((banner_height, rgb_img.shape[1], 3), dtype=np.uint8)
        banner[:] = (50, 50, 50)

        lstv_type = lstv_info['lstv_type'].upper()
        confidence = lstv_info.get('confidence_level', 'UNKNOWN')

        if lstv_type == 'LUMBARIZATION':
            text = f"âš  LSTV: LUMBARIZATION - {confidence} CONFIDENCE"
            banner_color = LSTV_COLORS['L6']
        elif lstv_type == 'SACRALIZATION':
            text = f"âš  LSTV: SACRALIZATION - {confidence} CONFIDENCE"
            banner_color = (255, 128, 0)
        elif lstv_type == 'S1_S2_DISC':
            text = f"âš  LSTV: S1-S2 DISC - {confidence} CONFIDENCE"
            banner_color = (255, 128, 0)
        else:
            text = f"âš  LSTV: ATYPICAL - {confidence} CONFIDENCE"
            banner_color = (255, 255, 0)

        cv2.putText(banner, text, (10, 28),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, banner_color, 2, cv2.LINE_AA)

        rgb_img = np.vstack([banner, rgb_img])

    return rgb_img


def extract_three_view_images(
    nifti_path,
    instance_path,
    semantic_path,
    output_dir,
    qa_dir,
    study_id,
    selector,
    analysis,
    use_semantic_optimization=True
):
    """Extract 3 views with optional semantic optimization"""
    try:
        nii = nib.load(nifti_path)
        instance_nii = nib.load(instance_path)

        mri_data = nii.get_fdata()
        instance_data = instance_nii.get_fdata().astype(int)

        semantic_data = None
        if semantic_path and semantic_path.exists():
            semantic_nii = nib.load(semantic_path)
            semantic_data = semantic_nii.get_fdata().astype(int)

        dims = mri_data.shape
        sag_axis = np.argmin(dims)

        if use_semantic_optimization and semantic_data is not None:
            print(f"    Using semantic optimization")
            voxel_spacing = nii.header.get_zooms()

            t12_slices = find_optimal_parasagittal_slices_semantic(
                semantic_data, instance_data, 'T12', voxel_spacing
            )

            l5_slices = find_optimal_parasagittal_slices_semantic(
                semantic_data, instance_data, 'L5', voxel_spacing
            )

            views = {
                'left': t12_slices['left'],
                'mid': l5_slices['left'],
                'right': t12_slices['right'],
            }

            print(f"    Rib densities - L:{t12_slices['left_density']} R:{t12_slices['right_density']}")
        else:
            print(f"    Using standard selection")
            slice_info = selector.get_three_slices(instance_data, sag_axis)
            views = {
                'left': slice_info['left'],
                'mid': slice_info['mid'],
                'right': slice_info['right'],
            }

        output_paths = {}

        for view_name, slice_idx in views.items():
            thickness = 15 if view_name in ['left', 'right'] else 5
            mri_slice = extract_slice(mri_data, sag_axis, slice_idx, thickness=thickness)
            instance_slice = extract_slice(instance_data, sag_axis, slice_idx, thickness=thickness)

            normalized = normalize_slice(mri_slice)

            output_path = output_dir / f"{study_id}_{view_name}.jpg"
            cv2.imwrite(str(output_path), normalized)
            output_paths[view_name] = output_path

            if qa_dir:
                labeled_img = create_labeled_overlay(normalized, instance_slice, analysis)
                qa_path = qa_dir / f"{study_id}_{view_name}_QA.jpg"
                cv2.imwrite(str(qa_path), labeled_img)

        return output_paths

    except Exception as e:
        print(f"  âœ— Image extraction failed: {e}")
        import traceback
        traceback.print_exc()
        return None


def upload_to_roboflow(image_path, study_id, roboflow_key, workspace, project):
    """Upload to Roboflow"""
    try:
        from roboflow import Roboflow
        rf = Roboflow(api_key=roboflow_key)
        workspace_obj = rf.workspace(workspace)
        project_obj = workspace_obj.project(project)
        project_obj.upload(
            image_path=str(image_path),
            split="train",
            tag_names=["lstv-candidate", "automated"],
            num_retry_uploads=3
        )
        return True
    except Exception as e:
        print(f"    Upload error: {e}")
        return False


def load_progress(progress_file):
    """Load progress"""
    if progress_file.exists():
        try:
            with open(progress_file, 'r') as f:
                return json.load(f)
        except:
            pass
    return {
        'processed': [],
        'flagged': [],
        'failed': [],
        'high_confidence': [],
        'medium_confidence': [],
        'low_confidence': [],
        'semantic_available': [],
        'semantic_missing': []
    }


def save_progress(progress_file, progress):
    """Save progress"""
    try:
        temp_file = progress_file.with_suffix('.json.tmp')
        with open(temp_file, 'w') as f:
            json.dump(progress, f, indent=2)
        temp_file.replace(progress_file)
    except:
        pass


# ============================================================================
# MAIN PROCESSING
# ============================================================================

def process_study(
    study_dir: Path,
    output_dirs: Dict[str, Path],
    series_df: Optional[pd.DataFrame],
    selector: SpineAwareSliceSelector,
    args
) -> Optional[Dict]:
    """Process a single study - CSV IS ABSOLUTE TRUTH"""
    study_id = study_dir.name

    try:
        # Get prioritized list of series to try (CSV IS TRUTH)
        series_candidates = select_best_series(study_dir, series_df, study_id)
        if series_candidates is None:
            return None

        # Handle both single series and list of series
        if not isinstance(series_candidates, list):
            series_candidates = [series_candidates]

        # Try each series - NO ORIENTATION VERIFICATION
        nifti_path = None
        selected_series = None

        for series_dir in series_candidates:
            nifti_candidate = output_dirs['nifti'] / f"sub-{study_id}_sequ-sag_T2w.nii.gz"

            if nifti_candidate.exists():
                # Use cached file without verification
                nifti_path = nifti_candidate
                selected_series = series_dir
                print(f"  Using cached NIfTI (series {series_dir.name})")
                break

            # Try converting this series - NO VERIFICATION (verify_sagittal=False)
            print(f"  Trying series {series_dir.name}...")
            nifti_candidate, orientation = convert_dicom_to_nifti(
                series_dir, nifti_candidate, study_id, verify_sagittal=False
            )

            if nifti_candidate is not None:
                print(f"    âœ“ Converted! Orientation: {orientation}")
                nifti_path = nifti_candidate
                selected_series = series_dir
                break
            else:
                print(f"    âœ— Conversion failed")

        if nifti_path is None:
            print(f"  âœ— Failed to convert any series")
            return None

        print(f"  âœ“ Using series: {selected_series.name}")

        # Run SPINEPS
        seg_outputs = run_spineps_dual_extraction(nifti_path, output_dirs['segmentations'])
        if seg_outputs is None:
            return None

        instance_path = seg_outputs['instance']
        semantic_path = seg_outputs.get('semantic')

        # Analyze
        print(f"  Analyzing...")
        analysis = analyze_segmentation(instance_path)
        if analysis is None:
            return None

        result = {
            'study_id': study_id,
            'series_id': selected_series.name,
            'vertebra_count': analysis['vertebra_count'],
            'is_lstv_candidate': analysis['is_lstv_candidate'],
            'lstv_type': analysis['lstv_type'],
            'lumbar_labels': str(analysis['lumbar_labels']),
            'confidence_score': analysis['confidence_score'],
            'confidence_level': analysis['confidence_level'],
            'has_semantic': semantic_path is not None,
        }

        # Extract images if LSTV
        if analysis['is_lstv_candidate']:
            confidence_level = analysis['confidence_level']
            confidence_score = analysis['confidence_score']

            print(f"  ðŸš© LSTV! Type={analysis['lstv_type']}, "
                  f"Confidence={confidence_level} ({confidence_score:.2f})")

            image_paths = extract_three_view_images(
                nifti_path,
                instance_path,
                semantic_path,
                output_dirs['images'],
                output_dirs['qa'],
                study_id,
                selector,
                analysis,
                use_semantic_optimization=(semantic_path is not None)
            )

            if image_paths:
                if confidence_score >= args.confidence_threshold:
                    if args.roboflow_key and args.roboflow_key != 'SKIP':
                        upload_success = 0
                        for view_name, image_path in image_paths.items():
                            if upload_to_roboflow(
                                image_path, f"{study_id}_{view_name}",
                                args.roboflow_key, args.roboflow_workspace,
                                args.roboflow_project
                            ):
                                upload_success += 1

                        print(f"  âœ“ Uploaded {upload_success}/3 views")
                    else:
                        print(f"  âœ“ Images saved (upload skipped)")
                else:
                    print(f"  âš  Images saved, upload skipped ({confidence_level} < threshold)")

        else:
            print(f"  âœ“ Normal ({analysis['vertebra_count']} lumbar)")

        return result

    except Exception as e:
        print(f"  âœ— Processing error: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Production LSTV Screening v3.0 - CSV IS ABSOLUTE TRUTH',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--mode', type=str, required=True,
                       choices=['diagnostic', 'trial', 'full'])
    parser.add_argument('--input_dir', type=str, required=True)
    parser.add_argument('--output_dir', type=str, required=True)
    parser.add_argument('--series_csv', type=str, required=True,
                       help='Path to train_series_descriptions.csv (REQUIRED)')
    parser.add_argument('--limit', type=int, default=None)
    parser.add_argument('--roboflow_key', type=str, default='SKIP')
    parser.add_argument('--roboflow_workspace', type=str, default='lstv-screening')
    parser.add_argument('--roboflow_project', type=str, default='lstv-candidates')
    parser.add_argument('--confidence_threshold', type=float, default=0.7)
    parser.add_argument('--verbose', action='store_true')

    args = parser.parse_args()

    # Validate series CSV exists
    series_csv_path = Path(args.series_csv)
    if not series_csv_path.exists():
        print(f"ERROR: Series CSV not found: {series_csv_path}")
        print(f"This file is REQUIRED to identify sagittal T2 series")
        sys.exit(1)

    # Set limits
    if args.mode == 'diagnostic':
        study_limit = args.limit or 5
    elif args.mode == 'trial':
        study_limit = args.limit or 50
    else:
        study_limit = args.limit

    # Setup directories
    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    output_dirs = {
        'nifti': output_dir / 'nifti',
        'segmentations': output_dir / 'segmentations',
        'images': output_dir / 'candidate_images',
        'qa': output_dir / 'qa_images',
    }

    for d in output_dirs.values():
        d.mkdir(exist_ok=True)

    # Load series CSV
    print(f"Loading series descriptions from: {series_csv_path}")
    series_df = load_series_descriptions(series_csv_path)
    if series_df is None:
        print(f"ERROR: Failed to load series CSV")
        sys.exit(1)

    # Initialize
    selector = SpineAwareSliceSelector()
    progress_file = output_dir / 'progress.json'
    progress = load_progress(progress_file)
    results_csv = output_dir / 'results.csv'

    # Get studies
    study_dirs = sorted([d for d in input_dir.iterdir() if d.is_dir()])
    if study_limit:
        study_dirs = study_dirs[:study_limit]

    print(f"\n{'='*80}")
    print(f"LSTV SCREENING v3.0 - {args.mode.upper()} MODE - CSV IS ABSOLUTE TRUTH")
    print(f"{'='*80}")
    print(f"Studies to process: {len(study_dirs)}")
    print(f"Already processed: {len(progress['processed'])}")
    print(f"Confidence threshold: {args.confidence_threshold}")
    print(f"Roboflow upload: {'Enabled' if args.roboflow_key != 'SKIP' else 'Disabled'}")
    print(f"{'='*80}\n")
    sys.stdout.flush()

    # Process
    for study_dir in tqdm(study_dirs, desc="Processing"):
        study_id = study_dir.name

        if study_id in progress['processed']:
            continue

        print(f"\n[{study_id}]")
        sys.stdout.flush()

        try:
            result = process_study(study_dir, output_dirs, series_df, selector, args)

            if result is None:
                progress['failed'].append(study_id)
            else:
                if result.get('has_semantic'):
                    progress['semantic_available'].append(study_id)
                else:
                    progress['semantic_missing'].append(study_id)

                if result['is_lstv_candidate']:
                    confidence_level = result['confidence_level']
                    if confidence_level == 'HIGH':
                        progress['high_confidence'].append(study_id)
                        progress['flagged'].append(study_id)
                    elif confidence_level == 'MEDIUM':
                        progress['medium_confidence'].append(study_id)
                    else:
                        progress['low_confidence'].append(study_id)

                df = pd.DataFrame([result])
                if results_csv.exists():
                    df.to_csv(results_csv, mode='a', header=False, index=False)
                else:
                    df.to_csv(results_csv, mode='w', header=True, index=False)

            progress['processed'].append(study_id)
            save_progress(progress_file, progress)

        except KeyboardInterrupt:
            print("\nâš ï¸  Interrupted!")
            save_progress(progress_file, progress)
            sys.exit(1)
        except Exception as e:
            print(f"  âœ— Unexpected error: {e}")
            progress['failed'].append(study_id)
            progress['processed'].append(study_id)
            save_progress(progress_file, progress)

    # Final report
    print(f"\n{'='*80}")
    print(f"COMPLETE - {args.mode.upper()} MODE")
    print(f"{'='*80}")
    print(f"Total processed: {len(progress['processed'])}")
    print(f"Failed: {len(progress['failed'])}")
    print()
    print(f"LSTV Candidates: {len(progress['flagged'])}")
    print(f"  HIGH confidence:   {len(progress['high_confidence'])}")
    print(f"  MEDIUM confidence: {len(progress['medium_confidence'])}")
    print(f"  LOW confidence:    {len(progress['low_confidence'])}")
    print()
    print(f"Semantic masks:")
    print(f"  Available: {len(progress['semantic_available'])}")
    print(f"  Missing:   {len(progress['semantic_missing'])}")
    print()
    print(f"Output: {output_dir}")
    print(f"Results: {results_csv}")
    print(f"{'='*80}\n")

    if args.mode == 'diagnostic':
        semantic_pct = len(progress['semantic_available']) / max(1, len(progress['processed'])) * 100
        print(f"ðŸ“Š DIAGNOSTIC SUMMARY:")
        print(f"   Semantic availability: {semantic_pct:.1f}%")
        if semantic_pct > 80:
            print(f"   âœ“ Excellent - use semantic optimization")
        elif semantic_pct > 50:
            print(f"   âš  Partial semantic coverage")
        else:
            print(f"   âœ— Low - fallback to standard selection")


if __name__ == "__main__":
    main()
